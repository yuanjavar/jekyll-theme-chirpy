---
layout: post
title: 分布式算法：Raft 是如何达成共识的？
category: arch
tags: [架构,distributed-algorithm,分布式算法]
description: Raft是如何达成共识的？
keywords: 分布式算法,Raft算法,共识算法
---

你好，我是Weiki，欢迎来到猿java。

在 [分布式算法：Paxos 是如何达成共识的？](https://www.yuanjava.cn/posts/paxos-protocol/) 这篇文章中，我们深入的讲解了
Paxos算法，知道了 Basic Paxos是如何达成共识以及存在哪些问题，Multi-Paxos在通过引入Leader节点解决了Basic Paxos 多 Proposer(提议者)带来的问题，但是对于如何选举 Leader 却没有详细的说明。
因此，今天我们就来分析一种 Multi-Paxos算法的经典实现 Raft算法。
算法。


> 提示：下面列举了 raft算法 的一些专业术语及其中文翻译，帮助大家理解
>
> term：任期；term counter：任期编号； heartbeat：心跳； timeout：超时

## 什么是 Raft算法

Raft 是英文"**R**eliable、**R**eplicated、**R**edundant、**A**nd **F**ault-**T**olerant"（“可靠、可复制、可冗余、可容错”）的首字母缩写， 它起源于 2013 年 斯坦福大学 Diego Ongaro 和 John Ousterhout 的博士论文《In Search of an Understandable Consensus Algorithm》。因为 Paxos 晦涩难懂且缺乏工程实现，所以作者要设计个既容易实现又利于学生学习的一致性算法。
它是一种用于替代 Paxos的共识算法。相比于 Paxos，Raft 的目标是提供更清晰的逻辑分工使得算法本身能被更好地理解，同时它安全性更高，并能提供一些额外的特性。Raft 能为在计算机集群之间部署有限状态机提供一种通用方法，并确保集群内的任意节点在某种状态转换上保持一致。

## 3种角色(身份/状态)

在Raft算法 中有 Leader(领导者)，Candidate(候选人)，Follower(追随者) 3种角色，也可以说成 3种身份或者状态，关于它们的说明如下：

**Leader(领导者)**

- 负责处理所有外部的请求，如果不是 Leader机器收到请求时，请求会被转到 Leader机器；
- 负责向 Follower(追随者) 同步心跳信息；
- 负责向 其他节点 同步日志复制信息；
- 同一任期，最多只能有一个 Leader；

**Candidate(候选人)**

- 主动发起 Leader(领导者)选举投票；
- 重置选举超时时间；
- 获取 大多数 Follower(追随者) 的投票后成为 Leader(领导者)；

**Follower(追随者)**

- 响应 Leader(领导者) 的 AppendEntries RPC 心跳请求；
- 响应 Candidate(候选人) 的 RequestVote RPC 投票请求；
- 接受 Leader(领导者) 和 Candidate(候选人) 角色切换成 Follower(追随者)；

三者的关系如下图：

![img.png](http://127.0.0.1:4000/assets/md/framework/raft-roles.png)

## 如何选举 Leader

在讲解 Leader选举之前，我们先来看下 Raft Safety的 2个约束：

> 1. 同一任期内每个节点最多只能投票 1次。
>
> 2. Candidate 只有获得大多数选票才能成为 Leader。画外音是：一个集群只能存在一个Leader。

带着这两个约束，为了更情景化的说明 Leader的选举过程，本文将以包含 节点A、节点B、节点C 3个节点组成的集群来进行演示。

**初始状态**

初始状态时，每个节点的角色都是 Follower(追随者)，Term任期编号为 0(假设任期编号从0开始)，并且每个节点都伴有一个随机倒计时(
假设节点A：100ms，节点B：150ms，节点C：180ms)，如下图：

![img.png](http://127.0.0.1:4000/assets/md/framework/raft-leader-init.png)

**投票请求**

因为节点A 的倒计时是 100ms，3 个节点中最小的。所以，节点A 最先结束倒计时被唤醒，成功晋升成 Candidate(候选人)，然后将自己的 Term counter
(任期编号) +1，同时为自己先投一票，再向其他的 Follower 发起 RequestVote RPC 请求投票，如下图：

![img.png](http://127.0.0.1:4000/assets/md/framework/raft-leader-req.png)

**投票响应**

Follower(追随者) 节点B，节点C 收到 Candidate(候选人)节点A 的 RequestVote Rpc 投票请求后，会做如下处理：
```text
if(在Term任期编号1的选举中已经投过票){
   忽略请求；
}else {
  将选票 投给 Candidate(候选人)节点A，并且将自己的任期编号设置为1；
}
```
这里假设 节点B和C 在任期编号为 1 的选举中没有投过票，所以把选票投给节点A，并且把自己的任期编号设置为 1，交互如下图：

![img.png](http://127.0.0.1:4000/assets/md/framework/raft-leader-resp.png)

**投票结束**

Candidate(候选人)节点A 在任期编号为 1 的选举内赢得了大多数的选票，成为本任期的 Leader(领导者)，为了维持自己的 Leader(领导者)
地位，Leader(领导者)节点A 需要不间断的给 Follower(追随者) 节点 B 和 C 发送心跳包，告诉他们自己还存活，让 节点 B 和 C 重置
随机倒计时，防止节点B和C重新发起投票。整体交互如下图：

![img.png](http://127.0.0.1:4000/assets/md/framework/raft-leader-heart.png)


到此，一个完整的 Leader选举过程就描述结束，该流程是不是和我们读书时代的选班长有异曲同工之妙？


不过，看完上面的选举描述，我们可能会想，假如集群中有 2个或者多个节点同时发起投票，整个过程会怎样了？

**多个 Candidate问题**

在上述 Leader选举的描述中我们可以发现，每个节点都有一个随机倒计时，因为节点被唤醒是随机的，这样大大降低了多个节点在同一时刻被唤醒成为 Candidate 的概率。
但是小概率的事件不代表不发生，因此我们以 2个节点同时被唤醒的场景来分析整个 Leader选举流程：

假如 节点 A 和B 的随机倒计时都是 100ms，这样两个节点就会同时被唤醒，成为 Candidate(候选人)，首先 节点 A 和 B 会分别为自己投一票，然后向其他节点请求投票，如果 节点 A 的投票请求先于节点 B 到达节点 C，
最终，节点 A 获取 2张选票，节点 B 获取 1张选票，因此，节点A 获取大多数选票成为 Leader，节点B 的角色会从 Candidate 退成Follower，整个交互如下图：

![img.png](http://127.0.0.1:4000/assets/md/framework/raft-leader-two-candidate.png)

**Split Vote 平分选票问题**

上述描述的都是基于"奇数个节点的集群"，如果集群中的节点是偶数个，结果又是怎样了，为了更好的说明问题，此处采用 4个节点的集群进行说明：

假设节点 A 和 B 的随机倒计时都是 100ms，这样两个节点就会同时被唤醒成为 Candidate(候选人)，首先节点 A 和 B 会分别为自己投一票，然后再向其他节点请求投票，因为节点 A 和 B 已为自己投票，
根据同一任期内最多投 1票的约束，节点 A 和 B 会拒绝给对方投票， 最终 节点 A 和 B 各自只能获取 2票，这里出现了一个经典的问题：Split Vote(平分票数)，该如何处理呢？

随机倒计时，在这种"平分选票"未选出 Leader 的情况下，所有节点会全部切换成 Follower 状态，重新设置唤醒倒计时，准备下一轮的选举。不过需要提醒的是选举的过程越长越增加了集群不可用的时长，整个交互如下图：

![img.png](http://127.0.0.1:4000/assets/md/framework/raft-leader-4-server.png)
![img.png](http://127.0.0.1:4000/assets/md/framework/raft-leader-4-server2.png)

通过"平分选票问题"，也侧面反应了日常生产中应部署 3,5,7这样的奇数节点，降低平分选票问题的出现。

**脑裂问题**

上文我们一直在强调：一个集群中最多只能有一个 Leader。假如，在一个集群内部发生网络分区，形成了 2个小集群，会不会出现 2个Leader？很有可能，因此我们来看看"脑裂问题"。

由A,B,C,D,E 5个节点组成的集群，假如内部出现了网络问题，节点 ABC 能够相互通信，节点 DE 能相互通信，ABC 和 DE 不能通信，这种，一个大集群分裂成两个小集群，每个小集群都能够选出 Leader，这就是我们说的"脑裂问题"，如下图：

![img.png](http://127.0.0.1:4000/assets/md/framework/raft-rupture.png)

Raft 是如何解决这种问题的？接下来我们进入「节点变更问题」篇幅。


## 节点变更问题

节点变更是分布式系统中很常见的问题，比如，服务器扩容需要增加机器，服务器缩容需要减少机器，或者出现节点故障需要变更机器，那么 Raft对于集群节点变更是如何处理来保证始终只有一个Leader的呢？

### 节点增加


### 节点减少


## 如何复制日志

## 如何保证安全

## 总结







从上面 Leader选举的过程，可以看出，Candidate 向 Follower 发起投票请求使用的是 RequestVote Rpc方式，Leader 和 Follower 维护心跳使用的是 AppendEntries RPC，
那么RequestVote Rpc 和 AppendEntries RPC 分别是什么呢？我们讲在下文进行讲解。
## 鸣谢

如果你觉得本文章有帮助，感谢转发给更多的好友，我们定将呈现更多的干货， 欢迎关注公众号：猿java

